---
title: "Feature engineering"
format: gfm
editor: visual
code-tools: true
execute: 
  warning: false
  echo: false
---

My summary of the [Feature Engineering book by Kuhn and Johnson](https://www.feat.engineering/).

```{r}
#| echo: false
#| output: false
library(caret)
library(colorspace)
library(FactoMineR)
library(ggdist)
library(ggthemes)
library(RColorBrewer)
# library(skimr)
library(tidymodels)
library(tidyverse)
library(vcd)

theme_set(theme_minimal())
```

```{r}
#| echo: false
#| output: false
library(doParallel)
cl <- makePSOCKcluster(parallel::detectCores() - 2)
registerDoParallel(cl)
```

### Supervised and unsupervised learning

Supervised data analysis involves identifying patterns between predictors and an identified outcome that is to be modeled or predicted, while unsupervised techniques are focused solely on identifying patterns among the predictors.

Exploratory data analysis is used to explore the data und uncover potential challenges by examining the characteristics of the predictors and the outcome variable as well as their relationships. This includes their distribution, interaction, correlation and any peculiarities which might make the modeling process challenging.

Predictive models are strictly supervised as we aim to find relationships between the predictors and the outcome. Unsupervised methods include cluster analysis, principal component analysis and similar tools. While both approaches are prone to overfitting, supervised learning is more inclined to finding erroneous patterns in the data.

### The Modeling Process

The process of developing a model is iterative. It is not uncommon to evaluate multiple and different approaches before a model is finalized. The modeling process begins with exploratory data analysis, where data is investigated. This might include simple summary statistics or identifying relationships between the predictors and the outcome. This process is iterative and includes repeated visualizations and analysis until the data is thoroughly understood. At this point several different modeling methods might be evaluated with the initial feature set. However, each model might come with its own set of hyperparameters that require tuning. Once these are tuned, each model is evaluated numerically on the training data to assess its performance. Several summary measures can be employed to understand the particular challenges of the data. At this point more EDA can be conducted on the results (e.g. on the identified misclassified cases). Following might be another round of feature engineering that might be necessary to compensate these challenges. At this point another round of model tuning on a limited number of models can be conducted. These models can than be evaluated on a new dataset to determine a final model.

### Bias vs. Variance

Variance describes how the data fluctuates when measured repeatedly. A model has high variance if small changes to the data cause a sizable change in the structure of the model. E.g. the sample mean has higher variance than its median. Examples of models with inherently *low variance* are *linear regression*, *logistic regression* and *partial least squares*. Contrary, *high-variance* models are those that strongly rely on single data points, such as *tree-based models*, *nearest neighbor models*, and *neural networks*. Model bias describes the flexibility of the model, i.e. its ability to generalize to unseen data. Thus, a high-bias model will not be able to make good predictions on data which distribution deviates from the data it has been trained on. Linear models have a high bias since they cannot describe non-linear patterns without further modifications.

The variance-bias trade-off is a common theme, as models with low bias demonstrate high variance - and vice versa. In many cases, models have parameters which allow to control how well they adjust to the data and thus allow to control the bias and variance of the results. For example, in a moving average model, which aims to predict temperature on a given day by using the average of the day within a certain window, a small moving window is more likely to be influenced by a single data point and thus display high variance, but will pick up local trends in the data (low bias). Using a bigger window would make the model less responsive to trends in the data (lower variance) but will result in a less sensitive model (high bias).

One way of achieving a low-variance, low-bias model, is to augment a low-variance model with appropriate representations of the data to decrease the bias. A linear model is linear *in the model parameters*, but adding polynomials is a good way of adapting the model to non-linear trends in the data. Similarly, feature selection techniques can significantly improve model performance by eliminating unnecessary predictors, which cause excess model variance.

### Experience-Driven Modeling and Empirically Driven Modeling

In many cases model performance can be improved by utilizing some kind of model-based feature elimination. This approach can be dangerous as data-driven approaches often tend to overfit the data. Additionally, they may be overly complex and thus be difficult to explain and/or rationalize. It is not uncommon for some conflicts between experience-driven and data-driven approaches to arise. For example when an unexpected, novel predictor, which shows strong relationship with the outcome is identified. Subject matter experts may have more confidence in the finding if the methods that are used to arrive at the discovery are stringent enough.

Modeling insights to consider: - There is almost never a single model fit or feature set that will immediately solve the problem. - The effect of feature sets can be much larger than the effect of different models. - The interplay between models and features is complex and sometimes unpredictable. - With the right set of predictors, different types of models can achieve comparable performance.

## Data Visualization

We will use the Chicago Transit Authority (CTA) “L” train system data to predict ridership numbers for the Clark/Lake stop. For time series data, predictors are often formed by lagging the data. For this application, when predicting day D, predictors were created using the lag–14 data from every station (e.g., ridership at day D−14). Other lags can also be added to the model, if necessary.

### Visualizations for Numeric Data

```{r}
data(Chicago)
```

Univariate visualizations are used to understand the distribution of a single variable. A few common univariate visualizations are box-and-whisker plots (i.e., box plot), violin plots, or histograms.

Because the foremost goal of modeling is to understand variation in the response, the first step should be to understand the distribution of the response. For a continuous response such as the ridership at the Clark/Lake station, it is important to understand if the response has a symmetric distribution, if the distribution has a decreasing frequency of larger observations (i.e., the distribution is skewed), if the distribution appears to be made up of two or more individual distributions (i.e., the distribution has multiple peaks or modes), or if there appears to be unusually low or high observations (i.e outliers).

Understanding the distribution of the outcome provides valuable insights about the expectations of model performance. If the model contains good predictors then the residuals from this model should have less variation than the variation of the outcome. Further, the distribution of the outcome can tell us whether the outcome should be transformed. For example, if the outcome follows a log-normal distribution then log-transforming would project it into a normal distribution, which often results in better model performance. Investigating the distribution of the outcome also can provide clues about which variables need to be included and/or how to transform them.

Let us look at the distribution of ridership from the Chicago dataset. The boxplot below shows some summary statistics in the form of the minimum , lower quartile, median, upper quartile and maximum value. For a symmetric distribution the distances between the median and the quartiles, as well as the whiskers would appear symmetric.

```{r ridership_boxplot}
#| echo: false
  ggplot(Chicago, aes("", Clark_Lake)) +
  geom_boxplot(fill = "pink", alpha = 0.5) +
    # ylim(0, 30) +
    theme(
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
    ) +
    ylab("Clark/Lake Rides (x1000)") +
    coord_flip()
```

A drawback of the boxplot is that it does not show the density of the data, i.e. whether a distribution has one or more peaks. From the histogram (a) we can see two peaks, which the boxplot is unable to capture. The violin plot (c) retains the information from the histogram. Additionally, the same information as in the boxplot can be added to it.

```{r}
#| echo: false
#| warning: false
y_hist <- 
  ggplot(Chicago, aes(Clark_Lake)) +   
  geom_histogram(binwidth = .7, col = "#D53E4F", fill = "#D53E4F", alpha = .5) +  
  xlab("Clark/Lake Rides (x1000)") +
  ylab("Frequency") +
  ggtitle("(a)") +
  xlim(-2,29) +
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  )

y_box <-
  ggplot(Chicago, aes(x = "", y = Clark_Lake)) +
  geom_boxplot(alpha = 0.2) +
  ylab("Clark/Lake Rides (x1000)") +
  ggtitle("(b)") +
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  coord_flip() +
  ylim(-2,29)

y_violin <-
  ggplot(Chicago, aes(x = "", y = Clark_Lake)) +
  geom_violin(alpha = 0.2) +
  ylab("Clark/Lake Rides (x1000)") +
  ggtitle("(c)") +
  theme(
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  coord_flip() +
  ylim(-2,29)

gridExtra::grid.arrange(y_hist, y_box, y_violin, nrow = 3)
```

A "raincloud" plot is an extension to the traditional boxplot, that includes a density plot and a distribution plot.

```{r raincloud}
#| echo: false

Chicago %>% 
  mutate(dow =  wday(date, label = TRUE, abbr = TRUE),
         year = year(date)) %>% 
  filter(year == 2016) %>% 
ggplot(aes(factor(dow), Clark_Lake, fill = dow)) +
  stat_halfeye(
    adjust = 0.5,
    justification = -0.2, #adjust position horizontally
    .width = 0,
    point_color = NA) +
  geom_boxplot(aes(fill = dow), 
               width = 0.12,
               outlier.color = NA) +
  stat_dots(side = "left",
            justification = 1.1,
            binwidth = NA) + #auto binwidth
  xlab("") +
  ylab("Ridership (x1000)")+
  ggtitle("Daily Ridership at the Clark/Lake Station in 2016") +
  scale_fill_manual(values = colorRampPalette(
   colors = brewer.pal(n = 7, "YlOrRd")[-1])(7)) +
  guides(
    fill = guide_legend(
      title = "",
      nrow = 1,
      byrow = TRUE)) +
  theme_hc() +
  theme(legend.position = "top") +
  coord_flip()
```

When the predictors are on the same scale, we can plot them as side-by-side boxplots of violin plots.

```{r}
#| echo: false
#| warning: false

Chicago %>% 
select("Austin":"California", date) %>% 
  pivot_longer(-date, names_to = "Station", values_to = "Rides") %>% 
  group_by(Station) %>% 
  summarise(iqr = IQR(Rides),
    y25 = quantile(Rides, 0.25),
    y50 = median(Rides),
    y75 = quantile(Rides, 0.75)) %>% 
  mutate(lower = y25 - 1.5 * iqr,
         upper = y75 + 1.5 * iqr,
         lower = ifelse(lower < 0, 0, lower)) %>% 
  arrange(y50) %>% 
  ggplot(aes(Station)) +
  geom_boxplot(aes(
    ymin = lower, 
    lower = y25, 
    middle = y50, 
    upper = y75, 
    ymax = upper),
    stat = "identity"
  ) +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )
```

As the number of predictors grows, the ability to visualize the individual distributions lessens and may be practically impossible. In this situation, a subset of predictors that are thought to be important can be examined using these techniques.

### Augmenting Visualizations through Facetting, Colors and Shapes

Facetting, colors and shapes are valuable tools for increasing dimensionality of plots.

Following intuition we partition the Clark/Lake ridership by working days/weekend. This distinction was not part of the original predictor set but it clearly is useful in explaining the dual peak in the distribution we saw before.

```{r scatterplot}
#| echo: false
#| warning: false

l10_breaks <- scales::trans_breaks("log10", function(x) 10^x)
l10_labels <- scales::trans_format("log10", scales::math_format(10^.x))

Chicago <- Chicago %>% 
  mutate(dow =  wday(date, label = TRUE, abbr = TRUE)) %>% 
  mutate(weekday = as.factor(if_else(dow %in% c("Sat", "Sun"),"Weekend", "Working Day")))

ggplot(Chicago, aes(Clark_Lake)) +
  geom_histogram(aes(fill  = dow, color = dow)) +
  scale_x_log10(breaks = l10_breaks, labels = l10_labels)+
  facet_wrap(~ weekday, nrow = 2, scales = "free_y") 
```

Still we can see data points which the distinction between working day and weekend cannot explain, i.e. the long left tail of the working day distribution. Finding a pattern, which explains the occurrence of these points would improve the predictive model.

### Scatter plots

If the goal is to predict ridership at the Clark/Lake station then, intuitively, we should take into account past ridership information. Since we already know that ridership on working day and on weekends follow different distributions, a one-day lag would be less useful for predicting ridership on Monday or Saturday. Hence, it makes more sense to use a week-based lag since the information occurs on the same day of the week. Because the primary interest is in predicting ridership two weeks in advance, we will create the 14-day lag in ridership.

```{r scatter}
ridership_scatter <- Chicago %>% 
  mutate(lag14 = lag(Clark_Lake, n = 14)) %>% 
  filter(!is.na(lag14)) %>% 
  ggplot(aes(Clark_Lake, lag14, color = weekday)) +
  geom_point(alpha = 0.5) +
  scale_color_manual(values = c("pink", "cyan")) +
  xlab("Current Day (x1000)") +
  ylab("14-day Lag (x1000)") +
  theme(legend.title = element_blank()) +
  coord_equal()

ridership_scatter
```

We can already recognize a linear relationship between the 14-day lagged and the current ridership. This indicates that the 14-day lag will be a very good predictor for the ridership. Additionally, there are two groups of data point that lie far off the the bulk of points on the diagonal line. Finding a feature that explains the position of these points will be a useful addition to the model.

### Heatmaps

```{r heatmap}
heatmap_data <- Chicago %>% 
  select(date, weekday, Clark_Lake) %>% 
  mutate(mmdd = format(as.Date(date), "%m-%d"),
         yyyy = format(as.Date(date), "%Y"),
         lt10 = if_else(Clark_Lake < 10 & weekday == "Working Day", 1, 0))

break_vals <- sort(unique(format(as.Date(Chicago$date[format(Chicago$date, "%d") == "01" | format(Chicago$date, "%d") == "15"]), "%m-%d")))

ggplot(heatmap_data, aes(yyyy, mmdd)) +
  geom_tile(aes(fill = lt10)) +
  scale_fill_gradient(low = "transparent", high = "red") +
  scale_y_discrete(
    breaks = break_vals
  ) +
  xlab("Year") +
  ylab("Month & Day") +
  theme_bw() + 
  theme(legend.position = "none")
```

The working days with low ridership follow a regular pattern across the years. We can deduct that these fall on national holidays. Considering this, we can account for this by creating in indicator variable.\
We will exclude some common US holidays and re-create the scatterplot from above.

```{r nonhol_scatter}
common_holidays <- 
  c("01-01", "01-02", "01-15", "07-04", 
    "07-03", "07-05", "10-06", "10-07", 
    "12-24", "12-25", "12-26", "12-31")

holiday_data <- Chicago %>% 
  mutate(mmdd = format(as.Date(date), "%m-%d"),
         Holiday = case_when(
           mmdd %in% common_holidays ~ "Holiday",
           .default = "Non-holiday"),
         Lag14_holiday = lag(Holiday, n = 14),
         Lag14_holiday = if_else(is.na(Lag14_holiday), "Non-holiday", Lag14_holiday )) %>% 
  select(!mmdd)

create_lags <- function(df, lag = 14, date = FALSE) {
  prefix <- ifelse(lag < 10, paste0("0", lag), lag)
  prefix <- paste0("Lag", prefix, "_")
  
  for (station in stations){
    name <- paste0(prefix, station)
    df <- df %>% 
      mutate(!!name := lag(!!sym(station), n = lag))
  }
  df
}

lagged_data <- create_lags(holiday_data, lag = 14)
  
nonhol_scatter <- lagged_data %>% 
  filter(Holiday == "Non-holiday" & Lag14_holiday == "Non-holiday") %>%
  ggplot(aes(Clark_Lake, Lag14_Clark_Lake, color = weekday)) +
  geom_point(alpha = 0.5) +
  scale_color_manual(values = c("pink", "cyan")) +
  xlab("Current Ridership (x1000)") +
  ylab("14-days lag (x1000)") +
  ggtitle("Without Common Holidays") +
  theme(legend.position = "none") +
  coord_equal()

hol_scatter <- ridership_scatter +
  ggtitle("With Holidays") +
  theme(legend.position = "none")

gridExtra::grid.arrange(hol_scatter, nonhol_scatter, ncol = 2)
```

### Correlation Matrix

A correlation matrix can be thought of as an extended version of a scatter plot, which represents the correlation between all variables, combined with a heat map indicating the degree of correlation.

```{r corrmap}
cor_mat <- 
  lagged_data %>% 
  mutate(year = format(as.Date(date), "%Y")) %>% 
  filter(year == "2016") %>%
  select(starts_with("Lag14"), weekday, Holiday) %>%
  filter(weekday == "Working Day" & 
           Holiday == "Non-holiday") %>%
  select(!c(weekday, Holiday, Lag14_holiday)) %>% 
  cor()

heatmap(as.matrix(cor_mat), symm = TRUE,  
        col = colorRampPalette(brewer.pal(9, "Blues"))(100))
legend("bottomleft", 
       legend = seq(0, 1, length.out = 9), 
       fill = colorRampPalette(brewer.pal(9, "Blues"))(11), 
       title = "Correlation", 
       cex = 0.8)
```

The heatmap shows high correlations for ridership across nearly all stations. Additionally, there is very high correlation (dark blue clusters) between some stations, indicating redundant information, which could be excluded.

### Line Plots

Line plots are a great way of visualizing data with a time component. They can help with identifying time-dependent patterns.

We can investigate how the ridership has changed over the years during the week and on weekends.

```{r lineplot}
#| echo: false

rider_data <- Chicago %>% 
  mutate(year = format(as.Date(date), "%Y"),
         month = month(date, abbr = TRUE, label = TRUE)) %>% 
  group_by(year, month, weekday) %>% 
  summarise(n = mean(ridership),
            .groups = "drop")

rider_data %>% 
  ggplot(aes(month, n)) +
  facet_wrap(~weekday, ncol = 2) +
  geom_line(aes(group = year, color = year), linewidth = 1.5) +
  ylab("Mean Ridership") +
  xlab("") +
  scale_color_manual(values = colorRampPalette(
    colors = brewer.pal(n = 9, "YlOrRd")[-1])(16)) + 
  guides(
    col = guide_legend(
      title = "",
      nrow = 2,
      byrow = TRUE)) +
  theme(legend.position = "top")

```

The line plot reveals that ridership has increased continuously from 2001 to 2016. There are seasonal trends in the data: the numger of passengers during the week increases from January through October then plummets each year. The weekend data is more heterogeneous in this regard, especially during the years 2006 and 2010.

When we look at the z-score of the proportional ridership for each year we discover that in the year 2008 more people chose to ride the train in the summer months compared to other years, while it was the opposite a year later in 2009.

```{r }

z_score <- function(x) {
  mu <-  mean(x)
  s <-  sd(x)
  z <- (x-mu)/s
  z
}

Chicago %>% 
  mutate(year = format(as.Date(date), "%Y"),
         month = month(date, abbr = TRUE, label = TRUE)) %>% 
  filter(weekday == "Weekend" &
           year %in% c(2006:2015)
           ) %>%
  group_by(year, month) %>% 
  summarise(n = mean(ridership),
            .groups = "drop") %>% 
  group_by(year) %>% 
  mutate(total = sum(n),
         prop = round(n/total * 100, 2)) %>% 
  ungroup() %>% 
  group_by(month) %>%
  mutate(z_sc = z_score(prop)) %>%
  ungroup() %>%
  ggplot(aes(month, z_sc)) +
  geom_line(aes(group = year, color = year), linewidth = 1.5, alpha = 0.5) +
  ylab("Z-Score Perfcentage Ridership") +
  xlab("") +
  guides(
    col = guide_legend(
      title = "",
      nrow = 2,
      byrow = TRUE
    )) +
  theme(legend.position = "top")

```

```{r gas_prices_plot}
gas_prices <- read_csv("data/Chicago_gas_prices.csv", col_types = "Dn")
str(gas_prices) 
  mutate(date = format(date, "%m/%d/%Y"))
  mutate(date = format(as.Date(date), "%Y-%m-%d"))
  
  
  
Chicago %>% 
  inner_join(gas_prices, by = join_by(date))
```

We could speculate that the gas price in 2008 was particularly high and then decreased in the following year. A plot of the mean monthly ridership vs. monthly gas price would test this hypothesis.

### Principal Component Analysis

PCA is another way to project multidimensional data onto a 2D or 3D space, going beyond visualizing with colors, shapes and/or facets.

PCA condenses the dataset by finding combinations of variables, which maximize the variability in the data.

## Visualizations for Categorical Variables

For the visualizations of categorical variables the [OKCupid dataset](https://github.com/rudeboybert/JSE_OkCupid) will be used. The dataset contains information on 50 000 profiles from the online dating site OKCupid. The goal is to predict whether the profiles author's occupation is in a STEM field.

```{r cupid_data}

profiles <- read_csv("data/okcupid/profiles_revised.csv")
```

```{r cupid_structure}
skim(profiles)
```

### Visualizing Relationships between Outcome and Predictors

If we want to visualize categorical variables, we might choose bar plots to represent counts, proportions and frequencies.

```{r}

profiles <- profiles %>% 
  mutate(job = if_else(job %in% c("science / tech / engineering", "medicine / health") , "stem", "other"))
```

```{r religion}

binom_stats <- function(x, ...) {
  x <- x$job[!is.na(x$job)]
  res <- prop.test(x = sum(x == "stem"), n = length(x), ...)
  data.frame(Proportion  = unname(res$estimate), 
             Lower = res$conf.int[1],
             Upper = res$conf.int[2])
}

job_rates <- profiles %>% 
    filter(!is.na(job)) %>% 
    count(job) %>% 
    mutate(prop = n / sum(n) * 100) 

stem_rate <- job_rates$prop[job_rates$job == "stem"]

religion_rates <- 
  profiles %>%
  group_by(religion) %>%
  mutate(religion = case_when(
    str_detect(religion, "agnosticism") ~ "agnosticism",
    str_detect(religion,"christianity") ~ "christianity",
    str_detect(religion,"atheism") ~ "atheism",
    str_detect(religion,"catholicism") ~ "catholicism",
    str_detect(religion,"hinduism") ~ "hinduism",
    str_detect(religion,"judaism") ~ "judaism",
    str_detect(religion,"islam") ~ "islam",
    str_detect(religion,"buddhism") ~ "buddhism",
    is.na(religion) ~ "missing", 
    .default = "other")) %>% 
  do(binom_stats(.)) %>%
  arrange(Proportion) %>%
  ungroup() %>%
  mutate(religion = reorder(factor(religion), Proportion))

profiles <- 
  profiles %>% 
  mutate(
    religion2 = case_when(
     str_detect(religion, "agnosticism") ~ "agnosticism",
     str_detect(religion, "christianity") ~ "christianity",
     str_detect(religion, "atheism") ~ "atheism",
     str_detect(religion, "catholicism") ~ "catholicism",
     str_detect(religion, "hinduism") ~ "hinduism",
     str_detect(religion, "judaism") ~ "judaism",
     str_detect(religion, "islam") ~ "islam",
     str_detect(religion, "buddhism") ~ "buddhism",
     is.na(religion) ~ "missing",
     .default = "other"),
    religion2 = factor(religion2, levels = as.character(religion_rates$religion))
  )

```

In order to understand whether religion has any influence on the outcome we might plot the ratio of STEM to other jobs for each religion.

```{r cat_var}
ggplot(profiles, aes(x = religion2, fill = job)) +
  geom_bar(position = position_dodge()) + 
  scale_fill_brewer(palette = "Paired") +
  xlab("") +
  theme(legend.position = "top", axis.text = element_text(size = 8))
```

However, it is difficult to recognize the differences in the ratio of STEM : other fields. Even when they are ordered from highest to lowest like in the plot above.

We could choose to visualize the relative frequencies of STEM profiles as stacked bar plots. Here, it is apparent that the proportion of STEM fields is highest among Hindus. By adding the global proportion of STEM profiles, we can identify those religions where the proportion is higher.

```{r cat_stacked_bar}
ggplot(profiles, aes(x = religion2, fill = job)) + 
  geom_bar(position = "fill") +
  geom_hline(yintercept = stem_rate / 100, 
             col = "darkred", alpha = .5, 
             linetype = 2, linewidth = 1) +
  scale_fill_brewer(palette = "Paired") +
  xlab("") + 
  ylab("Proportion") +
  theme(legend.position = "none", axis.text = element_text(size = 8)) 
```

Still, this does not take into account the uncertainty, which differs based on sample size (compare Islam to Agnosticism for example). In some way this is illustrated by the height of the respective bar, but in no way precise.

By plotting the proportion of STEM profiles and a 95% confidence interval we can illustrate the degree of uncertainty associated with each group, as indicated by the height of the error bar.

```{r cat_confint}
ggplot(religion_rates, aes(x = religion, y = Proportion)) +
  geom_hline(yintercept = stem_rate / 100, col = "darkred", alpha = .3, 
             linetype = 2, linewidth = 1) +
  geom_point() +
  geom_errorbar(aes(ymin = Lower, ymax = Upper), width = .1) +
  theme(axis.text = element_text(size = 8)) +
  xlab("") +
  ylim(0, 0.3)
```

While plotting summary statistics is not the cookie cutter solution to all problems, it does answer our question in this case. Since there are differences between religions, which are also different from pure chance, a relationship to the outcome variable can be assumed.

#### Visualizing Categorical Output with Numeric Predictors

As an example we will examine the relationship between the outcome and based on the heights. The heights follow a normal distribution and are extremely similar between the two groups.

```{r}
profiles %>% 
  filter(!is.na(height)) %>% 
  ggplot(aes(height, fill = "darkred")) +
   stat_halfeye(
    adjust = 0.5,
    justification = -0.2, #adjust position horizontally
    .width = 0,
    point_color = NA) +
   geom_boxplot(width = 0.12,
               outlier.color = NA) +
   stat_dots(side = "left",
            justification = 1.1,
            binwidth = NA) + #auto binwidth
   xlab("Height [in]") +
   ylab("")+
   ggtitle("Height Distribution of OKCupid Profiles") +
   theme_hc() +
   theme(legend.position = "none")
```

```{r cat_x_num}
profiles %>% 
  ggplot(aes(height)) +
  geom_histogram(aes(fill = job), color = "black") +
  facet_grid(job~.) +
  xlab("Height [inches]") +
  theme(legend.position = "none")
```

Height seems to be an unlikely predictor of the outcome variable, but the approach does not address the question directly. We directly answer this question we would fit (and visualize) a general additive model, which also gives us the confidence intervals for our prediction.

```{r gam}
#| eval: false

gam_data <-  profiles %>% 
  select(height, job) %>% 
  filter(height > 10) %>% 
  mutate(job = if_else(job == "stem", 1, 0))

gam_small_set <- gam_data %>% 
  distinct(height)

gam_model <- mgcv::gam(job ~ s(height), data = gam_data, family = binomial())

gam_small_set <- gam_small_set %>%
  mutate(
    link = predict(gam_model, gam_small_set, type = "link"),
    se = predict(gam_model, gam_small_set, type = "link", se.fit = TRUE)$se.fit,
    upper = link + qnorm(.975) * se,
    lower = link - qnorm(.975) * se,
    lower = binomial()$linkinv(lower),
    upper = binomial()$linkinv(upper),
    probability = binomial()$linkinv(link)
  )

ggplot(gam_small_set, aes(height)) + 
geom_line(aes(y = probability)) + 
geom_ribbon(aes(ymin = lower, ymax = upper), fill = "grey", alpha = .5) + 
geom_hline(yintercept = stem_rate / 100, col = "darkred", alpha = .5, lty = 2)  + 
theme_bw() + 
xlab("") 
```

The black line represents the probability of the logistic regression model, the confidence intervals are represented by the grey area around the fit and the horizontal lines represents the baseline probability of STEM profiles.

The probability to be STEM is generally below that of pure chance, except for a small window between ca. 65-75 inches. As was also apparent from the wider tails of the histograms of other occupations, the probability plummets below 55 inches and above 75 inches of height. The big confidence intervals, i.e. great uncertainty, are indicative of a small number of observations.

Height - at least on its own - is not a good predictor and thus not worth including in a model.

### Relationships Between Categorical Predictors

Before deciding how to use predictors that contain non-numeric data, it is critical to understand their characteristics and relationships with other predictors. 

Instead of using repeated hypothesis tests of two-way associations like the Chi-square statistic, we can use cross-tabulations in conjunction with visualizations.

We will use two variables from the OKCupid dataset: drug and alcohol consumption. Fist, we compute the contigency table and visualize the results as a mosaic plot.

```{r contingency_table}
profiles <- profiles %>% 
  mutate(across(c(drugs, drinks), ~if_else(is.na(.), "missing", .)))

dnd_table <- table("Drugs" = profiles$drugs, "Alcohol" = profiles$drinks) 
dnd_table
```

```{r mosaic_plot}
vcd::mosaic(
  t(dnd_table),
  highlighting = TRUE,
  highlighting_fill = colorspace::rainbow_hcl,
  margins = unit(c(6, 1, 1, 8), "lines"),
  labeling = labeling_border(
    rot_labels = c(90, 0, 0, 0),
    just_labels = c("left", "right",
                    "center",  "right"),
    offset_varnames = unit(c(3, 1, 1, 4), "lines")
  ),
  keep_aspect_ratio = FALSE
)
```

Omitting missing information in both categories, most people never consume drugs and consume alcohol socially.

While the mosaic plot depicts the frequency distributions, a *correspondence analysis* will help us uncover the relationships between the variables. The Chi-square test uses the expected cell counts under the assumption that there is no relationship between the variables and the deviations (residuals) from these expected values to calculate the test statistic. The Chi-square statistic is large when both variables are strongly associated. The correspondence analysis uses the residuals to determine *principle coordinates*, which represent the observed residuals.

Much like in PCA, each axis represents one principle coordinate and the percentage the amount of information the coordinate explains.

Categories near the origin represent the most common value. Less frequently observed levels are located further from the origin.

Values from the same variable, which are located in close proximity are **redundant** (similar). These can potentially be coerced into one value.

Values in close proximity which are from different variables indicate an **association**.

The strength of the correlation can be assessed by the angle between the lines from the origin\
(x =0, y =0) to the respective points. The wider the angle the more the correlation shifts into negative numbers.

```{r correspondance}

cor_ana <- FactoMineR::CA(dnd_table, graph = F)

ca_drug <- as.data.frame(cor_ana$row$coord)
ca_drug$Label <- rownames(ca_drug)
ca_drug$Variable <- "Drugs"

ca_alc <- as.data.frame(cor_ana$col$coord)
ca_alc$Label <- rownames(ca_alc)
ca_alc$Variable <- "Alcohol"

ca_rng <- extendrange(c(ca_alc$`Dim 1`, ca_alc$`Dim 2`))
ca_x <- paste0("Dimension #1 (",
               round(cor_ana$eig["dim 1", "percentage of variance"], 0),
               "%)")
ca_y <- paste0("Dimension #2 (",
               round(cor_ana$eig["dim 2", "percentage of variance"], 0),
               "%)")

ca_coord <- rbind(ca_drug, ca_alc)

ggplot(ca_coord, aes(x = `Dim 1`, y = `Dim 2`, col = Variable)) + 
  geom_vline(xintercept = 0) + 
  geom_hline(yintercept = 0) + 
  geom_text(aes(label = Label)) + 
  xlim(ca_rng) + 
  ylim(ca_rng) + 
  xlab(ca_x) + 
  ylab(ca_y) + 
  theme(legend.position = "top") +
  coord_equal()
```

Dimension 1 (x-axis) explains more than half of the Chi-square statistic. Values close to zero indicate the most frequent value, i.e. most people choose to be abstinent from drugs and consume alcohol in social settings. Sporadic drug use and regular alcohol consumption are less frequent and thus located away from zero on the x-axis. Since both values are very close to each other a specific association can be assumed. Another very strong association is seen for regular drug use and very frequent drinking.

The second dimension (y-axis) accounts for one third of the Chi-square statistic. Most of the variation in this dimension is explained by missing values. Since there is very small variability in the second dimension, it can be assumed that the the variables have similar results.

```{r ca_principle_plot}
ca_coord %>% 
  filter(rownames(ca_coord) %in%  
           c("often", "very often", "sometimes", "rarely")) %>% 
  ggplot(aes(x = `Dim 1`, y = `Dim 2`, col = Variable)) + 
  geom_segment(x = 0, 
               y = 0, 
               xend = ca_coord["often", "Dim 1"], 
               yend = ca_coord["often", "Dim 2"],
               color = "purple", alpha = 0.5,
               linetype = 3) +
  geom_segment(x = 0, 
               y = 0, 
               xend = ca_coord["very often", "Dim 1"], 
               yend = ca_coord["very often", "Dim 2"],
               color = "purple", alpha = 0.5,
               linetype = 3) +
  geom_segment(x = 0, 
               y = 0, 
               xend = ca_coord["sometimes", "Dim 1"], 
               yend = ca_coord["sometimes", "Dim 2"],
               color = "darkgreen", alpha = 0.5,
               linetype = 3) +
  geom_segment(x = 0, 
               y = 0, 
               xend = ca_coord["rarely", "Dim 1"], 
               yend = ca_coord["rarely", "Dim 2"],
               color = "darkgreen", alpha = 0.5,
               linetype = 3) +
  geom_vline(xintercept = 0) + 
  geom_hline(yintercept = 0) + 
  geom_text(aes(label = Label)) + 
  annotate("text", label = "small angle\n long lines",
            x = 1.15, y = 0.5, color = "purple") +
  annotate("text", label = "wide angle\n short lines",
            x = 0.25, y = -0.175, color = "darkgreen") +
  xlim(ca_rng) + 
  ylim(ca_rng) + 
  xlab(ca_x) + 
  ylab(ca_y) + 
  theme(legend.position = "top") +
  coord_equal()
```

## Post-Modeling Visualization

Predictions on the assessment data produced during resampling can be used to understand performance of a model and drive the next set of improvements. The Chicago train data will be used to illustrate this process.

We can use model performance metrics to identify those relationships which might be useful to incorporate into our model. The partial regression plot utilizes the residuals from two distinct models and plots them against each other in a scatter plot.

We will split the Chicago train data set into a training set and a testing set. Since the data has a temporal component to it, use a rolling origin forecasting average for cross validation.

```{r chicago_split}

```

## Data Preprocessing

Knowing the characteristics of the data is a crucial step in the modeling process. This encompasses variable distributions, missingness, relationships between variables as well as with the outcome and so on.

### Numeric Predictors

There are many potential issues that can arise with predictors on a continuous scale. Some can be mitigated by the choice of an appropriate model. Tree-based models construct relationships between the outcome and a numeric predictor based on the ranks of the values, not on the value itself. These are also immune to outliers and/or skewed distributions. Distance-based approaches like kNN or SVM are much more sensitive to outliers as well as predictors being on different scales.

Another common problem with continuous predictors is multicollinearity. Partial least squares is specifically designed to handle this problem, while multiple linear regression and neural networks are highly adversely affected.

### 1:1 Transformations

One of the many ways numeric predictors can be transformed to improve their utility in a model is to change their scale.

A good example is to scale a highly skewed distribution to a more uniform distribution.

```{r}
#| echo: false
data("segmentationData")
segmentationData$Cell <- NULL
segmentationData <- segmentationData[, c("EqSphereAreaCh1", "PerimCh1", "Class", "Case")]
names(segmentationData)[1:2] <- paste0("Predictor", LETTERS[1:2])

example_train <- subset(segmentationData, Case == "Train")
example_test  <- subset(segmentationData, Case == "Test")

example_train$Case <- NULL
example_test$Case  <- NULL

simple_trans_rec <- recipe(Class ~ ., data = example_train) %>%
  step_BoxCox(PredictorA, PredictorB) %>%
  prep(training = example_train)

simple_trans_test <- bake(simple_trans_rec, example_test)
pred_b_lambda <-
  tidy(simple_trans_rec, number = 1) %>% 
  filter(terms == "PredictorB") %>% 
  select(value)

bc_before <- ggplot(example_test, aes(x = PredictorB)) + 
  geom_histogram(bins = 35, col = "blue", fill = "blue", alpha = .6) + 
  xlab("Predictor B") + 
  ggtitle("Original scale")
bc_after <- ggplot(simple_trans_test, aes(x = PredictorB)) + 
  geom_histogram(bins = 35, col = "red", fill = "red", alpha = .6) + 
  xlab("Predictor B (inverse)") + 
  ggtitle("Inverse transformation")

gridExtra::grid.arrange(bc_before, bc_after, nrow = 2)
```

The Box-Cox transformation was originally designed to transform the dependent variable in order to fix violations of linear regression assumptions like non-normality of residuals or heteroskedasticity.

$$
y𝛌 = \frac{y𝛌-1}{𝛌} 𝛌≠0
$$

$$
y𝛌 = log(𝛌) 𝛌=0
$$

This is a supervised transformation since it is applied to the outcome. The transformation is estimated from the linear model residuals.

It was later adapted to scale predictors

$$
y𝛌 = \frac{y𝛌-1}{𝛌\bar{x}^{𝛌-1}} 𝛌≠0
$$

$$
y𝛌 = \bar{x}log(x) 𝛌=0
$$

where $\bar{x}$ is the geometric mean of the predictor and $𝛌$ is the transformation parameter estimated form the predictor data.

This type of transformation is called **power transformation** because the parameter is in the exponent. The Box-Cox transformation is very flexible in its ability to handle different distributions. Based on the value of $𝛌$ the transformation can be of different types:

log-transformation for $𝛌 = 0$

square-root transformation for $𝛌 = 0.5$

inverse for $𝛌 = -1$

The Box-Cox transformation results in an approximately symmetric distribution but it can only be applied to strictly positive data. The analogous Yeo-Johnson transformation can be applied to any numeric data.

Since these transformations, when applied to predictors, have no relation to the outcome (and thus are *unsupervised*), they do not guarantee that the model actually improves.

#### Logit-transformation

The logit-transformation is applied to variables which are bounded between 0 and 1, such as proportions. This changes the scale from 0-1 to values between negative and positive infinity. A smsll constant value can be added to avoid dividing by zero, as for values close to 0 or 1.

The inverse logit transformation can be applied to bring the variable values back to their original scale.

#### Scaling and Centering

Centering, i.e. substracting the mean or median (robust scaling) from each individual value, is a common technique that - when applied to each of the predictors - sets the mean of the predictors to be zero.

Scaling is the process of dividing each individual value of a predictor by the standard deviation. This ensures that the variables standard deviation becomes one.

Alternatively, range scaling uses a variables minimum and maximum values to scale the data to a set interval (usually between zero and one).

These transformations are mild and needed for methods which require all predictors to be on the same scale, like distance-based or penalty-applying models (lasso/ridge).

Note that the statistics necessary for these transformations (mean, median, sd, etc.) are estimated from the training data and applied to test data and/or new data.

#### Moving Averages

Data smoothing, i.e. a running mean or median, can be applied to data containing a **time or frequency effect.** A running mean of 5 would replace each value with the mean of the value itself, the two values before and the two values after it. The size of the moving window is used to adjust the degree of smoothing.

```{r running_mean}
#| echo: false
#| eval: false

```

Smoothing methods can be applied to any sequential predictor and/or outcome data. It is important to make sure that the test set predictor data are smoothed separately to avoid having the training set influence values in the test set (or new unknown samples).

Predictors with a right-skewed distributions should be investigated closely. While in some cases high values may simply be outliers and thus removed from the dataset, in other cases the underlying distribution may indeed be positively scaled, e.g. area in m². Skewness can be addressed by a simple log-transformation or a more complicated Box-Cox (for strictly positive) or Yeo-Johnson transformation, which scales the data to a more uniform distribution.

```{r transformations}
#| eval: false
#| echo: false
a <- ggplot()

b

c

gridExtra::grid.arrange(a, b, c, ncol = 3)


```

Multicollinearity between predictors can be visually inspected by plotting a correlation map. The correlation threshold is chosen at the analysts discretion and may need to be lowered or raised depending on the problem and the model.

### One:Many Transformations

Numeric predictors, much like categorical ones (see below) can be expanded into multiple numeric predictors to improve model performance.

#### Basis Expansions and Splines

A basis expansion can be achieved by deriving a function with linear combinations of a given predictor x. For a cubic expansion with the formula

$$
f(x) = \sum_{i=1}^3 𝛃_if_i(x) = 𝛃_1x + 𝛃_2x² + 𝛃_3x³
$$

the original predictor is expanded upon with its squared and cubed versions.

For complex relationships, this global basis expansion, i.e. applied to the predictor across all ranges, might not be sufficiently nuanced.

This can be seen in the relationship between the sale price (outcome) and lot sizes in the Ames housing data. A linear relationship is only seen in the range of $10^{3.75} - 10^{4.25}$ but not on either side of that range.

```{r ames_lot_price}
data("ames")

ames %>% 
  mutate(across(c(Lot_Area, Sale_Price), ~ log(., base = 10))) %>%
  ggplot(aes(Lot_Area, Sale_Price)) +
  geom_point(alpha = 0.175) +
  scale_x_log10() +
  scale_y_log10() +
  theme_bw()+
  ylab("log10(Sale Price)") +
  xlab("log10(Lot Area)")
           
```

An alternative method for basis expansion are polynomial splines. Here, polynomial (mostly cubic) expansion are applied to pre-defined regions ("knots") of the predictor. These functions are connected at the knots to give a continuous function. The number of knots controls the complexity of the function and the "wiggliness" of the curve. With only few knots the curve can capture more simple trends, while with a higher number of knots complicated functions can adapt well to the data. This can also result in overfitting. The knots are typically chosen based on the percentiles of the data so that the data is distributed uniformly across the regions. For example, for a spline with four regions the knots would be placed at 25, 50 and 75% percentiles.

Below is a representation of the approach using six splines. Each panel in (a) shows the basis function at the respective knot, indicated by the blue vertical lines. Notice that the basis function for the first three knots is 0 (the first function is typically taken as the intercept). For the areas 4 through 6 the functions appear to lay the emphasis on the data outside of the main bulk.

![](data/numeric-ames-ns-1.svg){width="1600" height="700"}

The final form of the spline function is shown in panel (b). A linear trend is noticable in the middle of the point cloud and much weaker relationships outside. The data points on the left side are not represented by the curve particularly well. Here, more knots might be appropriate. The of knots is a hyperparameter that can be tuned and the right number of knots can be determined by cross-validation.

*Smoothing splines* start with a knot at *every single data point* and use a regularization technique (similar to ridge regression) to determine which point should be considered as a knot.

*Generalized additive models* (GAMs) extend generalized linear models (GLMs) by allowing non-linear terms for individual predictors. However, GAMs cannot model interactions between predictors in a traditional sense as this sort of model can descibe very complex relationships between the predictors. *General Additive Mixed Models* can include interaction terms and random effects to describe temporal (repeated measures) and clustering components in the data.\
GAMs can model nonlinear functions for each predictor with varying complexities.

*Loess,* a supervised regression model, uses a weighted regression across a predictor to estimate nonlinear patterns in the data.

The multivariate adaptive regression spline (MARS) uses a hinge function transformation given as:

$$
h(x) = xI(x>0)
$$

where $I$ is a simple indicator function that is x when x is greater than 0, and 0 otherwise.

For the Ames data lot area, if the log(lot_area) is given as $x$ then $h(x - 3.75)$ will set all values of the lot area, which are smaller than 3.75, to zero and all above to $x-3.75$. With the two hinge functions $h(x - 3.75)$ and the inverse $h(3.75 - x)$ we can define knots in the data.

\`pasnip::mars\`

```{r}
left <- ames %>% 
  mutate(Lot_Area = log(Lot_Area, base = 10),
         area_mars = case_when(
           Lot_Area < 3.75 ~ 3.75 - Lot_Area,
           .default = 0
         )) %>% 
  ggplot(aes(Lot_Area, area_mars)) +
  geom_point() +
  scale_x_log10(name = "Lot Area") +
  theme_bw() +
  ylab("MARS Feature Value")

right <- ames %>% 
  mutate(Lot_Area = log(Lot_Area, base = 10),
         area_mars = case_when(
           Lot_Area > 3.75 ~ Lot_Area - 3.75,
           .default = 0
         )) %>% 
  ggplot(aes(Lot_Area, area_mars)) +
  geom_point() +
  scale_x_log10(name = "Lot Area") +
  theme_bw() +
  ylab("MARS Feature Value")

gridExtra::grid.arrange(left, right, ncol = 2)
```

With two hing function with knots at 3.75 and 4.25 we could recreate the non-linear function from before with separate linear trend in the region below a value of $10^{3.75}$, between  $10^{3.75}$ and $10^{4.25}$ , and above $10^{4.25}$.

These feature generating functions are known in *neural networks* as ReLU (recitfied linear unit) activation functions.

**Basis functions can be effective in modeling nonlinear patterns for a predictor but also very useful for exploratory data analysis. These types of visualizations can guide the model building process by choosing the right function (quadratic, segmented, etc.).**

#### **Binning Numeric Data**

There may be different apparent rationales for binning data, e.g.:

-   There might be a higher chance for a myocardial infarction for people above the age of 60. In such case one might want to create two categories: below 60 y/o and 60+ y/o.

-   Binning might simplify the analysis as no non-linear relationship has to be figured out.

Binning of the data may be accomplished in a number of ways. One can base the desired cut-off values on specific percentiles (median, quartiles, etc.) or ROC curves. Be cautious of the fact that "eye-balling" the break points has a higher chance of overfitting.

Discretizing numeric data is problematic for different reasons:

-   There is seldom an objective rationale for a specific cut-off value. In the example above, one can hardly argue that there would be a higher risk for people of age 59 than 60.

-   When there is no association between the numeric predictor and the outcome, binning increases the probability of "finding" (i.e. creating) an association.

-   It becomes harder for the model to find the true underlying trend.

Discretizing should be done as a last-resort and must be included inside a resampling procedure to find the appropriate break points.

## Encoding Categorical Predictors

### Encoding Predictors with Multiple Levels

When dealing with a large number of predictor categories, one-hot encoding would lead to a unnecessarily crowded predictor matrix. If some of the categories have only few observations, e.g. zip codes and population of densely populated urban vs. rural areas, some resamples might not include those rare instances and the corresponding dummy variables would contain exclusively zeros (so-called *zero-variance predictors*). For some models this will result in errors.

Zero-variance predictors can be handled by simply removing them. Although simple, this approach requires *a priori* knowledge of the parameters which will be included in the model.

Potential zero-variance predictors can be identified before dummy encoding. These predictors have few unique values and appear infrequently. The ratio of the most-occurring to the second-most occurring value is a good gauge. A ratio of 19 is a good starting point, which should be adjusted depending on the data.

When rare occurrences are not to be eliminated, they can be concatenated into a new category (e.g. "other") when their respective occurrences fall below a predefined threshold.

Hashing is another way to combine predictor categories. Here, the original values are mapped to a smaller set of hash values. These can have binary values (like dummy variables) or signed values with values -1, 0 and 1.

As an example we will map a subset of the user locations from the OKCupid data to 16 new hash features. Each text value corresponds to a unique hash integer value. These integers values are then mapped to their respective feature values using the formula *column = (integer mod 16) + 1*.

```{r feature_hashing}
library(FeatureHashing)

load("data/okcupid/okc.RData")
set.seed(1)
towns <- sample(unique(as.character(okc_train$where_town)), 16)

location <- 
  okc_train %>% 
  dplyr::select(where_town) %>% 
  distinct(where_town) %>% 
  arrange(where_town)
```

```{r binary_hashes}
binary_hashes <-
  hashed.model.matrix(
    ~ where_town,
    data = location,
    hash.size = 2 ^ 4,
    create.mapping = TRUE
  )

binary_df <- 
  binary_hashes %>% 
  as.matrix() %>% 
  as_tibble() %>% 
  bind_cols(location) %>% 
  dplyr::rename(town = where_town) %>% 
  dplyr::filter(town %in% towns) %>% 
  arrange(town)

binary_df
```

Notice that some hashes are populated only be zeros, e.g. hashes 5 and 6. This is because none of the hash integers created had a corresponding mod 16 remainder. These could be removed, but new values, which are not represented in the training set might end up in one of these columns.

Column12 has more than one category assigned to it. In statistical terms, these categories are said to be *aliased* or *confounded*. Minimizing aliasing is an important aspect of a contrast scheme.

To combat this, we can use a *signed* hash function*.* A zero indicates no association with the feature and +/-1 indicate different values of the original data. While a binary hash wold encode both categories assigned to the same column as 1, the signed hash would assign +1 and -1, respectively. In column 12, we can see how the value for "castro valley" has changed from 1 in the binary hash table above to -1. and now can be distinguished from "oakland" and "pinole" by a regression model. However, the values for "oakland" and "pinole" are still 1 and thus indistinguishable.

```{r signed_hashes}
signed_hashes <-
  hashed.model.matrix(
    ~ where_town,
    data = location,
    hash.size = 2 ^ 4,
    signed.hash = TRUE
  )

signed_df <- 
  signed_hashes %>% 
  as.matrix() %>% 
  as_tibble() %>% 
  bind_cols(location) %>% 
  dplyr::rename(town = where_town) %>% 
  dplyr::filter(town %in% towns) %>% 
  arrange(town)
signed_df
```

Thus, aliasing remains a challenge when hashing is applied to predictors as it harbors a set of problems:

-   Aliasing is oblivious to relationships between the categories, e.g. whether the categories assigned to the same hash value are in close proximity, or any other associations possible.

-   When multiple categories are coerced to one, significant predictor, it may be very difficult to understand the true effect.

-   Hashing functions disregard the associated frequencies of the respective categories. Thus, a very frequent category may be aliased with one which is infrequent. In this case, the more frequent value will have a stronger effect.

### Supervised Encoding Methods

Supervised encoding methods use the outcome data as a guide for encoding categorical predictors to numeric values. This is especially well suited for predictors with many levels and when new levels are anticipated to appear after modeling, i.e. with new data.

The first method is called *effect encoding* or *likelihood encoding*, in which the effect of each factor level on the outcome is measured and then used for numeric encoding. For a regression problem, this might be the mean or median outcome value for the respective categorical level. For a classification model a glm can be used to estimate the effect of each level by calculating the rate of occurrence of the outcome.

For example, in the OKCupid training data the rate of STEM profiles in Mountain View is 0.53. We can calculate the odd as *p/(1-p) = 0.53/(1 - 0.53) = 0.53/0.47 = 1.125.* As logistic regression models estimate the log odds of the outcome, we can use these as the encoded values. Note that this only works if only one categorical predictor is used in the model.\

```{r}
okc_train %>% 
  filter(where_town == "mountain_view") %>% 
  count(Class) %>% 
  mutate(p = round(n / sum(n), 2),
         o = round(p / (1-p), 2),
         log_o = round(log(o), 2))
```

The drawback of this method is that levels with just one value will result in a very large log odds value. To address this issue, we can use the quality of the data, i.e. sample size (or variance for numeric predictors), in order to apply a weight to the respective level in order to move its value towards an overall estimate like the mean or median. This so-called *shrinkage method* can also be used to move extreme log-odds values towards the middle of the distribution.\
One shrinkage method is Bayesian analysis, which uses a *prior distribution* (or short: *prior*) for the estimates. Almost any distribution can be used for the prior, based on expert knowledge or educated assumption. The prior is then blended with the observed data to form the *posterior distribution*.

For the OKCupid data, we choose a normal distribution with a standard deviation of 10 for the log-odds.\

```{r}
library(embed)

partial_recipe <- 
  recipe(Class ~ ., data = okc_train) %>%
  step_lencode_bayes(
    where_town,
    outcome = vars(Class),
    verbose = FALSE,
    options = list(
      chains = 5, 
      iter = 1000, 
      cores = min(parallel::detectCores(), 5),
      seed = 123
    )
  ) %>%
  prep()


okc_props <- 
  okc_train %>%
  group_by(where_town) %>%
  summarise(
    rate = mean(Class == "stem"),
    raw  = log(rate/(1-rate)),
    n = length(Class)
  ) %>%
  mutate(where_town = as.character(where_town))

okc_props
```

### Dummy Variables vs. Factors in Tree-Based Models

Tree-based models do not require encoding of categorical predictors.

[Kuhn and Johnson](https://bookdown.org/max/FES/categorical-trees.html) tested different models with dummy-encoded and as-is factored categorical variables. For each model a variety of different performance metrics were estimated using resampling as well as the total time to train and tune the model.

![](data/categorical-factors-vs-dummies-acc-1.svg){width="1600" height="400"}

The only significant differences detected were in the time spent training the model.

![](data/categorical-factors-vs-dummies-time-1.svg){width="1600" height="400"}

The recommendation is to leave the predictors as-is and if the model is promising, to also try dummy encoding.
