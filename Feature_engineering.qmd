---
title: "Feature engineering"
format: gfm
editor: visual
code-tools: true
execute: 
  warning: false
  echo: false
---

My summary of the [Feature Engineering book by Kuhn and Johnson](https://www.feat.engineering/).

```{r}
#| echo: false
#| output: false
# library(caret)
library(colorspace)
library(FactoMineR)
library(ggdist)
library(ggthemes)
library(RColorBrewer)
# library(skimr)
library(tidymodels)
library(tidyverse)
library(vcd)

theme_set(theme_minimal())
```

```{r}
#| echo: false
#| output: false
library(doParallel)
cl <- makePSOCKcluster(parallel::detectCores() - 2)
registerDoParallel(cl)
```

### Supervised and unsupervised learning

Supervised data analysis involves identifying patterns between predictors and an identified outcome that is to be modeled or predicted, while unsupervised techniques are focused solely on identifying patterns among the predictors.

Exploratory data analysis is used to explore the data und uncover potential challenges by examining the characteristics of the predictors and the outcome variable as well as their relationships. This includes their distribution, interaction, correlation and any peculiarities which might make the modeling process challenging.

Predictive models are strictly supervised as we aim to find relationships between the predictors and the outcome. Unsupervised methods include cluster analysis, principal component analysis and similar tools. While both approaches are prone to overfitting, supervised learning is more inclined to finding erroneous patterns in the data.

### The Modeling Process

The process of developing a model is iterative. It is not uncommon to evaluate multiple and different approaches before a model is finalized. The modeling process begins with exploratory data analysis, where data is investigated. This might include simple summary statistics or identifying relationships between the predictors and the outcome. This process is iterative and includes repeated visualizations and analysis until the data is thoroughly understood. At this point several different modeling methods might be evaluated with the initial feature set. However, each model might come with its own set of hyperparameters that require tuning. Once these are tuned, each model is evaluated numerically on the training data to assess its performance. Several summary measures can be employed to understand the particular challenges of the data. At this point more EDA can be conducted on the results (e.g. on the identified misclassified cases). Following might be another round of feature engineering that might be necessary to compensate these challenges. At this point another round of model tuning on a limited number of models can be conducted. These models can than be evaluated on a new dataset to determine a final model.

### Bias vs. Variance

Variance describes how the data fluctuates when measured repeatedly. A model has high variance if small changes to the data cause a sizable change in the structure of the model. E.g. the sample mean has higher variance than its median. Examples of models with inherently *low variance* are *linear regression*, *logistic regression* and *partial least squares*. Contrary, *high-variance* models are those that strongly rely on single data points, such as *tree-based models*, *nearest neighbor models*, and *neural networks*. Model bias describes the flexibility of the model, i.e. its ability to generalize to unseen data. Thus, a high-bias model will not be able to make good predictions on data which distribution deviates from the data it has been trained on. Linear models have a high bias since they cannot describe non-linear patterns without further modifications.

The variance-bias trade-off is a common theme, as models with low bias demonstrate high variance - and vice versa. In many cases, models have parameters which allow to control how well they adjust to the data and thus allow to control the bias and variance of the results. For example, in a moving average model, which aims to predict temperature on a given day by using the average of the day within a certain window, a small moving window is more likely to be influenced by a single data point and thus display high variance, but will pick up local trends in the data (low bias). Using a bigger window would make the model less responsive to trends in the data (lower variance) but will result in a less sensitive model (high bias).

One way of achieving a low-variance, low-bias model, is to augment a low-variance model with appropriate representations of the data to decrease the bias. A linear model is linear *in the model parameters*, but adding polynomials is a good way of adapting the model to non-linear trends in the data. Similarly, feature selection techniques can significantly improve model performance by eliminating unnecessary predictors, which cause excess model variance.

### Experience-Driven Modeling and Empirically Driven Modeling

In many cases model performance can be improved by utilizing some kind of model-based feature elimination. This approach can be dangerous as data-driven approaches often tend to overfit the data. Additionally, they may be overly complex and thus be difficult to explain and/or rationalize. It is not uncommon for some conflicts between experience-driven and data-driven approaches to arise. For example when an unexpected, novel predictor, which shows strong relationship with the outcome is identified. Subject matter experts may have more confidence in the finding if the methods that are used to arrive at the discovery are stringent enough.

Modeling insights to consider: - There is almost never a single model fit or feature set that will immediately solve the problem. - The effect of feature sets can be much larger than the effect of different models. - The interplay between models and features is complex and sometimes unpredictable. - With the right set of predictors, different types of models can achieve comparable performance.

## Data Visualization

We will use the Chicago Transit Authority (CTA) “L” train system data to predict ridership numbers for the Clark/Lake stop. For time series data, predictors are often formed by lagging the data. For this application, when predicting day D, predictors were created using the lag–14 data from every station (e.g., ridership at day D−14). Other lags can also be added to the model, if necessary.

### Visualizations for Numeric Data

```{r}
data(Chicago)
```

Univariate visualizations are used to understand the distribution of a single variable. A few common univariate visualizations are box-and-whisker plots (i.e., box plot), violin plots, or histograms.

Because the foremost goal of modeling is to understand variation in the response, the first step should be to understand the distribution of the response. For a continuous response such as the ridership at the Clark/Lake station, it is important to understand if the response has a symmetric distribution, if the distribution has a decreasing frequency of larger observations (i.e., the distribution is skewed), if the distribution appears to be made up of two or more individual distributions (i.e., the distribution has multiple peaks or modes), or if there appears to be unusually low or high observations (i.e outliers).

Understanding the distribution of the outcome provides valuable insights about the expectations of model performance. If the model contains good predictors then the residuals from this model should have less variation than the variation of the outcome. Further, the distribution of the outcome can tell us whether the outcome should be transformed. For example, if the outcome follows a log-normal distribution then log-transforming would project it into a normal distribution, which often results in better model performance. Investigating the distribution of the outcome also can provide clues about which variables need to be included and/or how to transform them.

Let us look at the distribution of ridership from the Chicago dataset. The boxplot below shows some summary statistics in the form of the minimum , lower quartile, median, upper quartile and maximum value. For a symmetric distribution the distances between the median and the quartiles, as well as the whiskers would appear symmetric.

```{r ridership_boxplot}
#| echo: false
  ggplot(Chicago, aes("", Clark_Lake)) +
  geom_boxplot(fill = "pink", alpha = 0.5) +
    # ylim(0, 30) +
    theme(
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
    ) +
    ylab("Clark/Lake Rides (x1000)") +
    coord_flip()
```

A drawback of the boxplot is that it does not show the density of the data, i.e. whether a distribution has one or more peaks. From the histogram (a) we can see two peaks, which the boxplot is unable to capture. The violin plot (c) retains the information from the histogram. Additionally, the same information as in the boxplot can be added to it.

```{r}
#| echo: false
#| warning: false
y_hist <- 
  ggplot(Chicago, aes(Clark_Lake)) +   
  geom_histogram(binwidth = .7, col = "#D53E4F", fill = "#D53E4F", alpha = .5) +  
  xlab("Clark/Lake Rides (x1000)") +
  ylab("Frequency") +
  ggtitle("(a)") +
  xlim(-2,29) +
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  )

y_box <-
  ggplot(Chicago, aes(x = "", y = Clark_Lake)) +
  geom_boxplot(alpha = 0.2) +
  ylab("Clark/Lake Rides (x1000)") +
  ggtitle("(b)") +
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  coord_flip() +
  ylim(-2,29)

y_violin <-
  ggplot(Chicago, aes(x = "", y = Clark_Lake)) +
  geom_violin(alpha = 0.2) +
  ylab("Clark/Lake Rides (x1000)") +
  ggtitle("(c)") +
  theme(
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  coord_flip() +
  ylim(-2,29)

gridExtra::grid.arrange(y_hist, y_box, y_violin, nrow = 3)
```

A "raincloud" plot is an extension to the traditional boxplot, that includes a density plot and a distribution plot.

```{r raincloud}
#| echo: false

Chicago %>% 
  mutate(dow =  wday(date, label = TRUE, abbr = TRUE),
         year = year(date)) %>% 
  filter(year == 2016) %>% 
ggplot(aes(factor(dow), Clark_Lake, fill = dow)) +
  stat_halfeye(
    adjust = 0.5,
    justification = -0.2, #adjust position horizontally
    .width = 0,
    point_color = NA) +
  geom_boxplot(aes(fill = dow), 
               width = 0.12,
               outlier.color = NA) +
  stat_dots(side = "left",
            justification = 1.1,
            binwidth = NA) + #auto binwidth
  xlab("") +
  ylab("Ridership (x1000)")+
  ggtitle("Daily Ridership at the Clark/Lake Station in 2016") +
  scale_fill_manual(values = colorRampPalette(
   colors = brewer.pal(n = 7, "YlOrRd")[-1])(7)) +
  guides(
    fill = guide_legend(
      title = "",
      nrow = 1,
      byrow = TRUE)) +
  theme_hc() +
  theme(legend.position = "top") +
  coord_flip()
```

When the predictors are on the same scale, we can plot them as side-by-side boxplots of violin plots.

```{r}
#| echo: false
#| warning: false

Chicago %>% 
select("Austin":"California", date) %>% 
  pivot_longer(-date, names_to = "Station", values_to = "Rides") %>% 
  group_by(Station) %>% 
  summarise(iqr = IQR(Rides),
    y25 = quantile(Rides, 0.25),
    y50 = median(Rides),
    y75 = quantile(Rides, 0.75)) %>% 
  mutate(lower = y25 - 1.5 * iqr,
         upper = y75 + 1.5 * iqr,
         lower = ifelse(lower < 0, 0, lower)) %>% 
  arrange(y50) %>% 
  ggplot(aes(Station)) +
  geom_boxplot(aes(
    ymin = lower, 
    lower = y25, 
    middle = y50, 
    upper = y75, 
    ymax = upper),
    stat = "identity"
  ) +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )
```

As the number of predictors grows, the ability to visualize the individual distributions lessens and may be practically impossible. In this situation, a subset of predictors that are thought to be important can be examined using these techniques.

### Augmenting Visualizations through Facetting, Colors and Shapes

Facetting, colors and shapes are valuable tools for increasing dimensionality of plots.

Following intuition we partition the Clark/Lake ridership by working days/weekend. This distinction was not part of the original predictor set but it clearly is useful in explaining the dual peak in the distribution we saw before.

```{r scatterplot}
#| echo: false
#| warning: false

l10_breaks <- scales::trans_breaks("log10", function(x) 10^x)
l10_labels <- scales::trans_format("log10", scales::math_format(10^.x))

Chicago <- Chicago %>% 
  mutate(dow =  wday(date, label = TRUE, abbr = TRUE)) %>% 
  mutate(weekday = as.factor(if_else(dow %in% c("Sat", "Sun"),"Weekend", "Working Day")))

ggplot(Chicago, aes(Clark_Lake)) +
  geom_histogram(aes(fill  = dow, color = dow)) +
  scale_x_log10(breaks = l10_breaks, labels = l10_labels)+
  facet_wrap(~ weekday, nrow = 2, scales = "free_y") 
```

Still we can see data points which the distinction between working day and weekend cannot explain, i.e. the long left tail of the working day distribution. Finding a pattern, which explains the occurrence of these points would improve the predictive model.

### Scatter plots

If the goal is to predict ridership at the Clark/Lake station then, intuitively, we should take into account past ridership information. Since we already know that ridership on working day and on weekends follow different distributions, a one-day lag would be less useful for predicting ridership on Monday or Saturday. Hence, it makes more sense to use a week-based lag since the information occurs on the same day of the week. Because the primary interest is in predicting ridership two weeks in advance, we will create the 14-day lag in ridership.

```{r scatter}
ridership_scatter <- Chicago %>% 
  mutate(lag14 = lag(Clark_Lake, n = 14)) %>% 
  filter(!is.na(lag14)) %>% 
  ggplot(aes(Clark_Lake, lag14, color = weekday)) +
  geom_point(alpha = 0.5) +
  scale_color_manual(values = c("pink", "cyan")) +
  xlab("Current Day (x1000)") +
  ylab("14-day Lag (x1000)") +
  theme(legend.title = element_blank()) +
  coord_equal()

ridership_scatter
```

We can already recognize a linear relationship between the 14-day lagged and the current ridership. This indicates that the 14-day lag will be a very good predictor for the ridership. Additionally, there are two groups of data point that lie far off the the bulk of points on the diagonal line. Finding a feature that explains the position of these points will be a useful addition to the model.

### Heatmaps

```{r heatmap}
heatmap_data <- Chicago %>% 
  select(date, weekday, Clark_Lake) %>% 
  mutate(mmdd = format(as.Date(date), "%m-%d"),
         yyyy = format(as.Date(date), "%Y"),
         lt10 = if_else(Clark_Lake < 10 & weekday == "Working Day", 1, 0))

break_vals <- sort(unique(format(as.Date(Chicago$date[format(Chicago$date, "%d") == "01" | format(Chicago$date, "%d") == "15"]), "%m-%d")))

ggplot(heatmap_data, aes(yyyy, mmdd)) +
  geom_tile(aes(fill = lt10)) +
  scale_fill_gradient(low = "transparent", high = "red") +
  scale_y_discrete(
    breaks = break_vals
  ) +
  xlab("Year") +
  ylab("Month & Day") +
  theme_bw() + 
  theme(legend.position = "none")
```

The working days with low ridership follow a regular pattern across the years. We can deduct that these fall on national holidays. Considering this, we can account for this by creating in indicator variable.\
We will exclude some common US holidays and re-create the scatterplot from above.

```{r nonhol_scatter}
common_holidays <- 
  c("01-01", "01-02", "01-15", "07-04", 
    "07-03", "07-05", "10-06", "10-07", 
    "12-24", "12-25", "12-26", "12-31")

holiday_data <- Chicago %>% 
  mutate(mmdd = format(as.Date(date), "%m-%d"),
         Holiday = case_when(
           mmdd %in% common_holidays ~ "Holiday",
           .default = "Non-holiday"),
         Lag14_holiday = lag(Holiday, n = 14),
         Lag14_holiday = if_else(is.na(Lag14_holiday), "Non-holiday", Lag14_holiday )) %>% 
  select(!mmdd)

create_lags <- function(df, lag = 14, date = FALSE) {
  prefix <- ifelse(lag < 10, paste0("0", lag), lag)
  prefix <- paste0("Lag", prefix, "_")
  
  for (station in stations){
    name <- paste0(prefix, station)
    df <- df %>% 
      mutate(!!name := lag(!!sym(station), n = lag))
  }
  df
}

lagged_data <- create_lags(holiday_data, lag = 14)
  
nonhol_scatter <- lagged_data %>% 
  filter(Holiday == "Non-holiday" & Lag14_holiday == "Non-holiday") %>%
  ggplot(aes(Clark_Lake, Lag14_Clark_Lake, color = weekday)) +
  geom_point(alpha = 0.5) +
  scale_color_manual(values = c("pink", "cyan")) +
  xlab("Current Ridership (x1000)") +
  ylab("14-days lag (x1000)") +
  ggtitle("Without Common Holidays") +
  theme(legend.position = "none") +
  coord_equal()

hol_scatter <- ridership_scatter +
  ggtitle("With Holidays") +
  theme(legend.position = "none")

gridExtra::grid.arrange(hol_scatter, nonhol_scatter, ncol = 2)
```

### Correlation Matrix

A correlation matrix can be thought of as an extended version of a scatter plot, which represents the correlation between all variables, combined with a heat map indicating the degree of correlation.

```{r corrmap}
cor_mat <- 
  lagged_data %>% 
  mutate(year = format(as.Date(date), "%Y")) %>% 
  filter(year == "2016") %>%
  select(starts_with("Lag14"), weekday, Holiday) %>%
  filter(weekday == "Working Day" & 
           Holiday == "Non-holiday") %>%
  select(!c(weekday, Holiday, Lag14_holiday)) %>% 
  cor()

heatmap(as.matrix(cor_mat), symm = TRUE,  
        col = colorRampPalette(brewer.pal(9, "Blues"))(100))
legend("bottomleft", 
       legend = seq(0, 1, length.out = 9), 
       fill = colorRampPalette(brewer.pal(9, "Blues"))(11), 
       title = "Correlation", 
       cex = 0.8)
```

The heatmap shows high correlations for ridership across nearly all stations. Additionally, there is very high correlation (dark blue clusters) between some stations, indicating redundant information, which could be excluded.

### Line Plots

Line plots are a great way of visualizing data with a time component. They can help with identifying time-dependent patterns.

We can investigate how the ridership has changed over the years during the week and on weekends.

```{r lineplot}
#| echo: false

rider_data <- Chicago %>% 
  mutate(year = format(as.Date(date), "%Y"),
         month = month(date, abbr = TRUE, label = TRUE)) %>% 
  group_by(year, month, weekday) %>% 
  summarise(n = mean(ridership),
            .groups = "drop")

rider_data %>% 
  ggplot(aes(month, n)) +
  facet_wrap(~weekday, ncol = 2) +
  geom_line(aes(group = year, color = year), linewidth = 1.5) +
  ylab("Mean Ridership") +
  xlab("") +
  scale_color_manual(values = colorRampPalette(
    colors = brewer.pal(n = 9, "YlOrRd")[-1])(16)) + 
  guides(
    col = guide_legend(
      title = "",
      nrow = 2,
      byrow = TRUE)) +
  theme(legend.position = "top")

```

The line plot reveals that ridership has increased continuously from 2001 to 2016. There are seasonal trends in the data: the numger of passengers during the week increases from January through October then plummets each year. The weekend data is more heterogeneous in this regard, especially during the years 2006 and 2010.

When we look at the z-score of the proportional ridership for each year we discover that in the year 2008 more people chose to ride the train in the summer months compared to other years, while it was the opposite a year later in 2009.

```{r }

z_score <- function(x) {
  mu <-  mean(x)
  s <-  sd(x)
  z <- (x-mu)/s
  z
}

Chicago %>% 
  mutate(year = format(as.Date(date), "%Y"),
         month = month(date, abbr = TRUE, label = TRUE)) %>% 
  filter(weekday == "Weekend" &
           year %in% c(2006:2015)
           ) %>%
  group_by(year, month) %>% 
  summarise(n = mean(ridership),
            .groups = "drop") %>% 
  group_by(year) %>% 
  mutate(total = sum(n),
         prop = round(n/total * 100, 2)) %>% 
  ungroup() %>% 
  group_by(month) %>%
  mutate(z_sc = z_score(prop)) %>%
  ungroup() %>%
  ggplot(aes(month, z_sc)) +
  geom_line(aes(group = year, color = year), linewidth = 1.5, alpha = 0.5) +
  ylab("Z-Score Perfcentage Ridership") +
  xlab("") +
  guides(
    col = guide_legend(
      title = "",
      nrow = 2,
      byrow = TRUE
    )) +
  theme(legend.position = "top")

```

We could speculate that the gas price in 2008 was particularly high and then decreased in the following year. A plot of the mean monthly ridership vs. monthly gas price would test this hypothesis.

### Principal Component Analysis

PCA is another way to project multidimensional data onto a 2D or 3D space, going beyond visualizing with colors, shapes and/or facets.

PCA condenses the dataset by finding combinations of variables, which maximize the variability in the data.

## Visualizations for Categorical Variables

For the visualizations of categorical variables the [OKCupid dataset](https://github.com/rudeboybert/JSE_OkCupid) will be used. The dataset contains information on 50 000 profiles from the online dating site OKCupid. The goal is to predict whether the profiles author's occupation is in a STEM field.

```{r cupid_data}

profiles <- read_csv("data/profiles_revised.csv")
```

```{r cupid_structure}
skim(profiles)
```

### Visualizing Relationships between Outcome and Predictors

If we want to visualize categorical variables, we might choose bar plots to represent counts, proportions and frequencies.

```{r}

profiles <- profiles %>% 
  mutate(job = if_else(job %in% c("science / tech / engineering", "medicine / health") , "stem", "other"))
```

```{r religion}

binom_stats <- function(x, ...) {
  x <- x$job[!is.na(x$job)]
  res <- prop.test(x = sum(x == "stem"), n = length(x), ...)
  data.frame(Proportion  = unname(res$estimate), 
             Lower = res$conf.int[1],
             Upper = res$conf.int[2])
}

job_rates <- profiles %>% 
    filter(!is.na(job)) %>% 
    count(job) %>% 
    mutate(prop = n / sum(n) * 100) 

stem_rate <- job_rates$prop[job_rates$job == "stem"]

religion_rates <- 
  profiles %>%
  group_by(religion) %>%
  mutate(religion = case_when(
    str_detect(religion, "agnosticism") ~ "agnosticism",
    str_detect(religion,"christianity") ~ "christianity",
    str_detect(religion,"atheism") ~ "atheism",
    str_detect(religion,"catholicism") ~ "catholicism",
    str_detect(religion,"hinduism") ~ "hinduism",
    str_detect(religion,"judaism") ~ "judaism",
    str_detect(religion,"islam") ~ "islam",
    str_detect(religion,"buddhism") ~ "buddhism",
    is.na(religion) ~ "missing", 
    .default = "other")) %>% 
  do(binom_stats(.)) %>%
  arrange(Proportion) %>%
  ungroup() %>%
  mutate(religion = reorder(factor(religion), Proportion))

profiles <- 
  profiles %>% 
  mutate(
    religion2 = case_when(
     str_detect(religion, "agnosticism") ~ "agnosticism",
     str_detect(religion, "christianity") ~ "christianity",
     str_detect(religion, "atheism") ~ "atheism",
     str_detect(religion, "catholicism") ~ "catholicism",
     str_detect(religion, "hinduism") ~ "hinduism",
     str_detect(religion, "judaism") ~ "judaism",
     str_detect(religion, "islam") ~ "islam",
     str_detect(religion, "buddhism") ~ "buddhism",
     is.na(religion) ~ "missing",
     .default = "other"),
    religion2 = factor(religion2, levels = as.character(religion_rates$religion))
  )

```

In order to understand whether religion has any influence on the outcome we might plot the ratio of STEM to other jobs for each religion.

```{r cat_var}
ggplot(profiles, aes(x = religion2, fill = job)) +
  geom_bar(position = position_dodge()) + 
  scale_fill_brewer(palette = "Paired") +
  xlab("") +
  theme(legend.position = "top", axis.text = element_text(size = 8))
```

However, it is difficult to recognize the differences in the ratio of STEM : other fields. Even when they are ordered from highest to lowest like in the plot above.

We could choose to visualize the relative frequencies of STEM profiles as stacked bar plots. Here, it is apparent that the proportion of STEM fields is highest among Hindus. By adding the global proportion of STEM profiles, we can identify those religions where the proportion is higher.

```{r cat_stacked_bar}
ggplot(profiles, aes(x = religion2, fill = job)) + 
  geom_bar(position = "fill") +
  geom_hline(yintercept = stem_rate / 100, 
             col = "darkred", alpha = .5, 
             linetype = 2, linewidth = 1) +
  scale_fill_brewer(palette = "Paired") +
  xlab("") + 
  ylab("Proportion") +
  theme(legend.position = "none", axis.text = element_text(size = 8)) 
```

Still, this does not take into account the uncertainty, which differs based on sample size (compare Islam to Agnosticism for example). In some way this is illustrated by the height of the respective bar, but in no way precise.

By plotting the proportion of STEM profiles and a 95% confidence interval we can illustrate the degree of uncertainty associated with each group, as indicated by the height of the error bar.

```{r cat_confint}
ggplot(religion_rates, aes(x = religion, y = Proportion)) +
  geom_hline(yintercept = stem_rate / 100, col = "darkred", alpha = .3, 
             linetype = 2, linewidth = 1) +
  geom_point() +
  geom_errorbar(aes(ymin = Lower, ymax = Upper), width = .1) +
  theme(axis.text = element_text(size = 8)) +
  xlab("") +
  ylim(0, 0.3)
```

While plotting summary statistics is not the cookie cutter solution to all problems, it does answer our question in this case. Since there are differences between religions, which are also different from pure chance, a relationship to the outcome variable can be assumed.

#### Visualizing Categorical Output with Numeric Predictors

As an example we will examine the relationship between the outcome and based on the heights. The heights follow a normal distribution and are extremely similar between the two groups.

```{r}
profiles %>% 
  filter(!is.na(height)) %>% 
  ggplot(aes(height, fill = "darkred")) +
   stat_halfeye(
    adjust = 0.5,
    justification = -0.2, #adjust position horizontally
    .width = 0,
    point_color = NA) +
   geom_boxplot(width = 0.12,
               outlier.color = NA) +
   stat_dots(side = "left",
            justification = 1.1,
            binwidth = NA) + #auto binwidth
   xlab("Height [in]") +
   ylab("")+
   ggtitle("Height Distribution of OKCupid Profiles") +
   theme_hc() +
   theme(legend.position = "none")
```

```{r cat_x_num}
profiles %>% 
  ggplot(aes(height)) +
  geom_histogram(aes(fill = job), color = "black") +
  facet_grid(job~.) +
  xlab("Height [inches]") +
  theme(legend.position = "none")
```

Height seems to be an unlikely predictor of the outcome variable, but the approach does not address the question directly. We directly answer this question we would fit (and visualize) a general additive model, which also gives us the confidence intervals for our prediction.

```{r gam}
#| eval: false

gam_data <-  profiles %>% 
  select(height, job) %>% 
  filter(height > 10) %>% 
  mutate(job = if_else(job == "stem", 1, 0))

gam_small_set <- gam_data %>% 
  distinct(height)

gam_model <- mgcv::gam(job ~ s(height), data = gam_data, family = binomial())

gam_small_set <- gam_small_set %>%
  mutate(
    link = predict(gam_model, gam_small_set, type = "link"),
    se = predict(gam_model, gam_small_set, type = "link", se.fit = TRUE)$se.fit,
    upper = link + qnorm(.975) * se,
    lower = link - qnorm(.975) * se,
    lower = binomial()$linkinv(lower),
    upper = binomial()$linkinv(upper),
    probability = binomial()$linkinv(link)
  )

ggplot(gam_small_set, aes(height)) + 
geom_line(aes(y = probability)) + 
geom_ribbon(aes(ymin = lower, ymax = upper), fill = "grey", alpha = .5) + 
geom_hline(yintercept = stem_rate / 100, col = "darkred", alpha = .5, lty = 2)  + 
theme_bw() + 
xlab("") 
```

The black line represents the probability of the logistic regression model, the confidence intervals are represented by the grey area around the fit and the horizontal lines represents the baseline probability of STEM profiles.

The probability to be STEM is generally below that of pure chance, except for a small window between ca. 65-75 inches. As was also apparent from the wider tails of the histograms of other occupations, the probability plummets below 55 inches and above 75 inches of height. The big confidence intervals, i.e. great uncertainty, are indicative of a small number of observations.

Height - at least on its own - is not a good predictor and thus not worth including in a model.

### Relationships Between Categorical Predictors

Before deciding how to use predictors that contain non-numeric data, it is critical to understand their characteristics and relationships with other predictors. 

Instead of using repeated hypothesis tests of two-way associations like the Chi-square statistic, we can use cross-tabulations in conjunction with visualizations.

We will use two variables from the OKCupid dataset: drug and alcohol consumption. Fist, we compute the contigency table and visualize the results as a mosaic plot.

```{r contingency_table}
profiles <- profiles %>% 
  mutate(across(c(drugs, drinks), ~if_else(is.na(.), "missing", .)))

dnd_table <- table("Drugs" = profiles$drugs, "Alcohol" = profiles$drinks) 
dnd_table
```

```{r mosaic_plot}
vcd::mosaic(
  t(dnd_table),
  highlighting = TRUE,
  highlighting_fill = colorspace::rainbow_hcl,
  margins = unit(c(6, 1, 1, 8), "lines"),
  labeling = labeling_border(
    rot_labels = c(90, 0, 0, 0),
    just_labels = c("left", "right",
                    "center",  "right"),
    offset_varnames = unit(c(3, 1, 1, 4), "lines")
  ),
  keep_aspect_ratio = FALSE
)
```

Omitting missing information in both categories, most people never consume drugs and consume alcohol socially.

While the mosaic plot depicts the frequency distributions, a *correspondence analysis* will help us uncover the relationships between the variables. The Chi-square test uses the expected cell counts under the assumption that there is no relationship between the variables and the deviations (residuals) from these expected values to calculate the test statistic. The Chi-square statistic is large when both variables are strongly associated. The correspondence analysis uses the residuals to determine *principle coordinates*, which represent the observed residuals.

Much like in PCA, each axis represents one principle coordinate and the percentage the amount of information the coordinate explains.

Categories near the origin represent the most common value. Less frequently observed levels are located further from the origin.

Values from the same variable, which are located in close proximity are **redundant** (similar). These can potentially be coerced into one value.

Values in close proximity which are from different variables indicate an **association**.

The strength of the correlation can be assessed by the angle between the lines from the origin \
(x =0, y =0) to the respective points. The wider the angle the more the correlation shifts into negative numbers.

```{r correspondance}

cor_ana <- FactoMineR::CA(dnd_table, graph = F)

ca_drug <- as.data.frame(cor_ana$row$coord)
ca_drug$Label <- rownames(ca_drug)
ca_drug$Variable <- "Drugs"

ca_alc <- as.data.frame(cor_ana$col$coord)
ca_alc$Label <- rownames(ca_alc)
ca_alc$Variable <- "Alcohol"

ca_rng <- extendrange(c(ca_alc$`Dim 1`, ca_alc$`Dim 2`))
ca_x <- paste0("Dimension #1 (",
               round(cor_ana$eig["dim 1", "percentage of variance"], 0),
               "%)")
ca_y <- paste0("Dimension #2 (",
               round(cor_ana$eig["dim 2", "percentage of variance"], 0),
               "%)")

ca_coord <- rbind(ca_drug, ca_alc)

ggplot(ca_coord, aes(x = `Dim 1`, y = `Dim 2`, col = Variable)) + 
  geom_vline(xintercept = 0) + 
  geom_hline(yintercept = 0) + 
  geom_text(aes(label = Label)) + 
  xlim(ca_rng) + 
  ylim(ca_rng) + 
  xlab(ca_x) + 
  ylab(ca_y) + 
  theme(legend.position = "top") +
  coord_equal()
```

Dimension 1 (x-axis) explains more than half of the Chi-square statistic. Values close to zero indicate the most frequent value, i.e. most people choose to be abstinent from drugs and consume alcohol in social settings. Sporadic drug use and regular alcohol consumption are less frequent and thus located away from zero on the x-axis. Since both values are very close to each other a specific association can be assumed. Another very strong association is seen for regular drug use and very frequent drinking.

The second dimension (y-axis) accounts for one third of the Chi-square statistic. Most of the variation in this dimension is explained by missing values. Since there is very small variability in the second dimension, it can be assumed that the the variables have similar results.

```{r}
ca_coord %>% 
  filter(rownames(ca_coord) %in%  
           c("often", "very often", "sometimes", "rarely")) %>% 
  ggplot(aes(x = `Dim 1`, y = `Dim 2`, col = Variable)) + 
  geom_segment(x = 0, 
               y = 0, 
               xend = ca_coord["often", "Dim 1"], 
               yend = ca_coord["often", "Dim 2"],
               color = "purple", alpha = 0.5,
               linetype = 3) +
  geom_segment(x = 0, 
               y = 0, 
               xend = ca_coord["very often", "Dim 1"], 
               yend = ca_coord["very often", "Dim 2"],
               color = "purple", alpha = 0.5,
               linetype = 3) +
  geom_segment(x = 0, 
               y = 0, 
               xend = ca_coord["sometimes", "Dim 1"], 
               yend = ca_coord["sometimes", "Dim 2"],
               color = "darkgreen", alpha = 0.5,
               linetype = 3) +
  geom_segment(x = 0, 
               y = 0, 
               xend = ca_coord["rarely", "Dim 1"], 
               yend = ca_coord["rarely", "Dim 2"],
               color = "darkgreen", alpha = 0.5,
               linetype = 3) +
  geom_vline(xintercept = 0) + 
  geom_hline(yintercept = 0) + 
  geom_text(aes(label = Label)) + 
  annotate("text", label = "small angle\n long lines",
            x = 1.15, y = 0.5, color = "purple") +
  annotate("text", label = "wide angle\n short lines",
            x = 0.25, y = -0.175, color = "darkgreen") +
  xlim(ca_rng) + 
  ylim(ca_rng) + 
  xlab(ca_x) + 
  ylab(ca_y) + 
  theme(legend.position = "top") +
  coord_equal()
```

## Post-Modeling Visualization

## Data Preprocessing

Knowing the characteristics of the data is a crucial step in the modeling process. This encompasses variable distributions, missingness, relationships between variables as well as with the outcome and so on.

### Numeric Predictors

There are many potential issues that can arise with predictors on a continuous scale. Some can be mitigated by the choice of an appropriate model. Tree-based models construct relationships between the outcome and a numeric predictor based on the ranks of the values, not on the value itself. These are also immune to outliers and/or skewed distributions. Distance-based approaches like kNN or SVM are much more sensitive to outliers as well as predictors being on different scales.

Another common problem with continuous predictors is multicollinearity. Partial least squares is specifically designed to handle this problem, while multiple linear regression and neural networks are highly adversely affected.

```{r}
#| echo: false
data("segmentationData")
segmentationData$Cell <- NULL
segmentationData <- segmentationData[, c("EqSphereAreaCh1", "PerimCh1", "Class", "Case")]
names(segmentationData)[1:2] <- paste0("Predictor", LETTERS[1:2])

example_train <- subset(segmentationData, Case == "Train")
example_test  <- subset(segmentationData, Case == "Test")

example_train$Case <- NULL
example_test$Case  <- NULL

simple_trans_rec <- recipe(Class ~ ., data = example_train) %>%
  step_BoxCox(PredictorA, PredictorB) %>%
  prep(training = example_train)

simple_trans_test <- bake(simple_trans_rec, example_test)
pred_b_lambda <-
  tidy(simple_trans_rec, number = 1) %>% 
  filter(terms == "PredictorB") %>% 
  select(value)

bc_before <- ggplot(example_test, aes(x = PredictorB)) + 
  geom_histogram(bins = 35, col = "blue", fill = "blue", alpha = .6) + 
  xlab("Predictor B") + 
  ggtitle("Original scale")
bc_after <- ggplot(simple_trans_test, aes(x = PredictorB)) + 
  geom_histogram(bins = 35, col = "red", fill = "red", alpha = .6) + 
  xlab("Predictor B (inverse)") + 
  ggtitle("Inverse transformation")

gridExtra::grid.arrange(bc_before, bc_after, nrow = 2)
```

The Box-Cox transformation was originally designed to transform the dependent variable in order to fix violations of linear regression assumptions like non-normality of residuals or heteroskedasticity.

$$
y𝛌 = \frac{y𝛌-1}{𝛌} 𝛌≠0
$$

$$
y𝛌 = log(𝛌) 𝛌=0
$$

This is a supervised transformation since it is applied to the outcome. The transformation is estimated from the linear model residuals.

It was later adapted to scale predictors

$$
y𝛌 = \frac{y𝛌-1}{𝛌\bar{x}^{𝛌-1}} 𝛌≠0
$$

$$
y𝛌 = \bar{x}log(x) 𝛌=0
$$

where $\bar{x}$ is the geometric mean of the predictor and $𝛌$ is the transformation parameter estimated form the predictor data.

This type of transformation is called **power transformation** because the parameter is in the exponent. The Box-Cox transformation is very flexible in its ability to handle different distributions. Based on the value of $𝛌$ the transformation can be of different types:

log-transformation for $𝛌 = 0$

square-root transformation for $𝛌 = 0.5$

inverse for $𝛌 = -1$

The Box-Cox transformation results in an approximately symmetric distribution but it can only be applied to strictly positive data. The analogous Yeo-Johnson transformation can be applied to any numeric data.

Since these transformations, when applied to predictors, have no relation to the outcome (and thus are *unsupervised*), they do not guarantee that the model actually improves.

#### Logit-transformation

The logit-transformation is applied to variables which are bounded between 0 and 1, such as proportions. This changes the scale from 0-1 to values between negative and positive infinity. A smsll constant value can be added to avoid dividing by zero, as for values close to 0 or 1.

The inverse logit transformation can be applied to bring the variable values back to their original scale.

#### Scaling and Centering

Centering, i.e. substracting the mean or median (robust scaling) from each individual value, is a common technique that - when applied to each of the predictors - sets the mean of the predictors to be zero.

Scaling is the process of dividing each individual value of a predictor by the standard deviation. This ensures that the variables standard deviation becomes one.

Alternatively, range scaling uses a variables minimum and maximum values to scale the data to a set interval (usually between zero and one).

These transformations are mild and needed for methods which require all predictors to be on the same scale, like distance-based or penalty-applying models (lasso/ridge).

Note that the statistics necessary for these transformations (mean, median, sd, etc.) are estimated from the training data and applied to test data and/or new data.

#### Moving Averages

Data smoothing, i.e. a running mean or median, can be applied to data containing a **time or frequency effect.** A running mean of 5 would replace each value with the mean of the value itself, the two values before and the two values after it. The size of the moving window is used to adjust the degree of smoothing.

```{r running_mean}
#| echo: false
#| eval: false

```

Smoothing methods can be applied to any sequential predictor and/or outcome data. It is important to make sure that the test set predictor data are smoothed separately to avoid having the training set influence values in the test set (or new unknown samples).

Predictors with a right-skewed distributions should be investigated closely. While in some cases high values may simply be outliers and thus removed from the dataset, in other cases the underlying distribution may indeed be positively scaled, e.g. area in m². Skewness can be addressed by a simple log-transformation or a more complicated Box-Cox (for strictly positive) or Yeo-Johnson transformation, which scales the data to a more uniform distribution.

```{r transformations}
#| eval: false
#| echo: false
a <- ggplot()

b

c

gridExtra::grid.arrange(a, b, c, ncol = 3)


```

Multicollinearity between predictors can be visually inspected by plotting a correlation map. The correlation threshold is chosen at the analysts discretion and may need to be lowered or raised depending on the problem and the model.
